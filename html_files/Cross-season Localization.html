<head>
<body bgcolor=white background=w8b.gif lang=JA link=blue vlink=purple
style='tab-interval:42.0pt;text-justify-trim:punctuation'>

<div class=WordSection1>

<p><b><span lang=EN-US style='font-size:24.0pt'>Leveraging Image-based Prior in
Cross-Season Place Recognition</span></b></p>

<p><b><span lang=EN-US>Abstract</span></b><span lang=EN-US>-In this <span
class=GramE>paper,</span> we address the challenging problem of single-view
cross-season place recognition. A new approach is proposed for compact discriminative
scene descriptor that helps in coping with changes in appearance in the <span
class=SpellE>environment.We</span> focus on a simple effective strategy that
uses objects whose appearance remain the same across seasons as valid <span
class=SpellE>landmarks.Unlike</span> popular bag-of-words (BoW) scene
descriptors that rely on a library of vector quantized visual features, our
descriptor is based on a library of raw image data (e.g., visual experience
shared by colleague robots, publicly available photo collections from Google <span
class=SpellE>StreetView</span>), and directly mines it to identify landmarks
(i.e., image patches) that effectively explain an input query/database image.
The discovered landmarks are then compactly described by their pose and shape
(i.e., library image ID, and bounding boxes) and used as a compact
discriminative scene descriptor for the input image. We collected a dataset of
single-view images across seasons with annotated ground truth, and evaluated
the effectiveness of our scene description framework by comparing its
performance to that of previous BoW approaches, and by applying an advanced
Naive Bayes Nearest neighbor (NBNN) image-to-class distance measure.</span></p>

<p style='margin-bottom:12.0pt'><span lang=EN-US>Members: Taisho Tsukamoto,
Kanji Tanaka<b><o:p></o:p></b></span></p>

<p style='margin-bottom:12.0pt'><b><span lang=EN-US>Relevant Publication:<br>
</span></b><span lang=EN-US><br>
Leveraging Image-Based Prior in Cross-Season Place Recognition</span>　<span
lang=EN-US><br>
Robotics and Automation (ICRA), 2015 IEEE International Conference on<br>
Ando Masatoshi, <span class=SpellE>Chokushi</span> <span class=SpellE>Yuuto</span>,
Tanaka Kanji, <span class=SpellE>Yanagihara</span> <span class=SpellE>Kentaro</span><br>
<a href="https://dblp.org/rec/bibtex/conf/icra/AndoCTY15">Bibtex source</a>, <a
href="icra2015_ando0530.pdf">Document PDF</a><br>
<br>
<b>Acknowledgements: </b>This work is supported in part by JSPS KAKENHI
Grant-in-Aid for Young Scientists (B) 23700229, and for Scientific Research (C)
26330297.<br style='mso-special-character:line-break'>
<![if !supportLineBreakNewLine]><br style='mso-special-character:line-break'>
<![endif]></span></p>

<p><b><span lang=EN-US>Members</span></b><span lang=EN-US> </span>　　　　<span
class=SpellE><span lang=EN-US>Chokushi</span></span><span lang=EN-US> <span
class=SpellE>Yuuto</span> Tanaka Kanji <span class=SpellE>Yanagihara</span> <span
class=SpellE>Kentaro</span></span></p>

<p><span lang=EN-US><o:p>&nbsp;</o:p></span></p>

<p style='margin-bottom:12.0pt'><b><span lang=EN-US>Cross season university
campus dataset</span></b><span lang=EN-US><br>
<br>
<span class=GramE>The</span> cross season dataset consists of 19,263 images
taken around a university campus, using a hand-held camera as a monocular
vision sensor. The images have been manually annotated to generate ground
truth. The datasets have been collected across four seasons over a year and
cover all the four seasons, and each of query, database, and library images
(i.e., consists of 3 x 3 x image collections).<br>
<br>
DOWNLOAD: <a
href="http://rc.his.u-fukui.ac.jp/cross_season_university_campus.zip">cross_season_university_campus.zip</a><br>
<br>
<b>Description of files<span class=GramE>:</span></b><br>
<br>
<span class=SpellE>cross_season</span>/<br>
&nbsp;SP/<br>
&nbsp;&nbsp;&nbsp;query/<br>
&nbsp;&nbsp;&nbsp;db/<br>
&nbsp;&nbsp;&nbsp;library/<br>
&nbsp;&nbsp;&nbsp;<span class=SpellE>gt</span>/<br>
&nbsp;SU/<br>
&nbsp;&nbsp;&nbsp;query/<br>
&nbsp;&nbsp;&nbsp;db/<br>
&nbsp;&nbsp;&nbsp;library/<br>
&nbsp;&nbsp;&nbsp;<span class=SpellE>gt</span>/<br>
&nbsp;AU/<br>
&nbsp;&nbsp;&nbsp;query/<br>
&nbsp;&nbsp;&nbsp;db/<br>
&nbsp;&nbsp;&nbsp;library/<br>
&nbsp;&nbsp;&nbsp;<span class=SpellE>gt</span>/<br>
&nbsp;WI/<br>
&nbsp;&nbsp;&nbsp;query/<br>
&nbsp;&nbsp;&nbsp;db/<br>
&nbsp;&nbsp;&nbsp;library/<br>
&nbsp;&nbsp;&nbsp;<span class=SpellE>gt</span>/<br>
<br>
Directory containing images four different seasons (SP/, SU/, AU/, WI/) and
ground truth files (<span class=SpellE>gt</span>/). The ground truth files have
the following layout<span class=GramE>:</span><br>
Every line consists of [QID] [AID] [BID] [EID]<br>
QID and AID are the image ids of a query image and the corresponding ground
truth database image. BID and EID define the area [BID,EID] of the image ids of
relevant database images that can be used as positive training data for the
query image (not used in the current paper).<br>
<br style='mso-special-character:line-break'>
<![if !supportLineBreakNewLine]><br style='mso-special-character:line-break'>
<![endif]></span></p>

<p align=center style='text-align:center'><span lang=EN-US><br>
<br>
src="../images_file/Cross-season%20Localization.files/image002.jpg" v:shapes="図_x0020_13"><![endif]></span></span></p>

<p><span class=GramE><span lang=EN-US>Fig. 1.</span></span><span lang=EN-US>
Single-view cross-season place recognition. The appearance of a place may vary
depending on geometric (e.g., viewpoint trajectories and object configuration)
and photometric conditions (e.g., illumination).Such changes in appearance lead
to difficulties in scene matching, and thereby increasing the requirement for a
highly discriminative, compact scene descriptor. In this figure, the panels
(top-left, top-right, bottom-left,bottom-right) shows visual images acquired in
autumn (AU:2013/10), winter(WI:2013/12), spring (SP:2014/4), and summer
(SU:2014/7), respectively.</span></p>

<p><span lang=EN-US style='mso-no-proof:yes'>
</v:shape><![endif]--><![if !vml]><img border=0 width=723 height=383
src="../images_file/Cross-season%20Localization.files/image004.jpg" v:shapes="図_x0020_19"><![endif]></span><span
lang=EN-US><br style='mso-special-character:line-break'>
<![if !supportLineBreakNewLine]><br style='mso-special-character:line-break'>
<![endif]></span></p>

<p align=center style='text-align:center'><span lang=EN-US>mining visual
phrases for landmark discovery</span></p>

<p><span lang=EN-US style='mso-no-proof:yes'>
</v:shape><![endif]--><![if !vml]><img border=0 width=648 height=406
src="../images_file/Cross-season%20Localization.files/image006.jpg" v:shapes="図_x0020_22"><![endif]></span></p>

<p align=center style='text-align:center'><span lang=EN-US>landmark comparison
for scene matching</span></p>

<p><span lang=EN-US>Fig. 2. System overview: proposing, verifying and
retrieving landmarks for cross-season place recognition. The proposed framework
consists of three distinct steps: (1) landmarks are proposed by patch-level
saliency evaluation(red boxes in </span>“<span lang=EN-US>Query</span>”<span
lang=EN-US>), (2) landmarks are verified by mining the image prior to find
similar patterns (red boxes in </span>“<span lang=EN-US>Mined image</span>”<span
lang=EN-US>), and (3) landmarks are retrieved by using the
bag-of-bounding-boxes scene descriptors (colored boxes in the bottom figure).</span></p>

<p align=center style='text-align:center'><span lang=EN-US style='mso-no-proof:
yes'>
</v:shape><![endif]--><![if !vml]><img border=0 width=706 height=514
src="../images_file/Cross-season%20Localization.files/image008.jpg" v:shapes="図_x0020_25"><![endif]></span></p>

<p align=center style='text-align:center'><span lang=EN-US>Fig. 3. Experimental
environments and viewpoint paths.</span></p>

<p><span lang=EN-US><br>
<span style='mso-no-proof:yes'><img border=0 width=733 height=684
id="_x0000_i1032" src=FIG4.PNG></span><br>
<span style='mso-no-proof:yes'>
</v:shape><![endif]--><![if !vml]><img border=0 width=564 height=520
src="../images_file/Cross-season%20Localization.files/image010.jpg" v:shapes="図_x0020_28"><![endif]></span><br>
<br>
Fig. 4. Datasets. Image datasets are collected for various types of scenes and
across seasons (top). The dataset consists of three datasets of query,database,
and library images collected during four different seasons over a year
(bottom).</span></p>

<p style='margin-bottom:12.0pt'><span lang=EN-US><o:p>&nbsp;</o:p></span></p>

<p><span lang=EN-US style='mso-no-proof:yes'>
<![if !vml]><img border=0 width=560 height=476
src="../images_file/Cross-season%20Localization.files/image012.jpg" v:shapes="図_x0020_31"><![endif]></span></p>

<p><span lang=EN-US><br>
Fig. 5. Examples of scene retrievals. From left to right, each panel shows a
query image, the ground truth image, the database image top-ranked by the BoW
method and by the proposed method.</span></p>

<p><span lang=EN-US><span style='mso-no-proof:yes'><img border=0 width=869
height=504 id="_x0000_i1029" src=process.png></span></span></p>

<p><span lang=EN-US style='mso-no-proof:yes'>
<![if !vml]><img border=0 width=937 height=543
src="../images_file/Cross-season%20Localization.files/image014.jpg" v:shapes="図_x0020_34"><![endif]></span><span
lang=EN-US><br>
Fig. 6. Examples of proposing, verifying and retrieving landmarks. From left to
right, input image, saliency image, landmark proposal (blue bounding box),
mined library image, landmark discovered w.r.t. the library image</span>’<span
lang=EN-US>s coordinate, and the top-ranked database image.</span></p>

<p><span lang=EN-US style='mso-no-proof:yes'>
<![if !vml]><img border=0 width=714 height=396
src="../images_file/Cross-season%20Localization.files/image016.jpg" v:shapes="図_x0020_37"><![endif]></span></p>

<p><span lang=EN-US><o:p>&nbsp;</o:p></span></p>

<p align=center style='text-align:center'><span lang=EN-US><br>
Fig. 7. Frequency of library images.</span></p>

<p><span lang=EN-US><o:p>&nbsp;</o:p></span></p>

<p align=center style='text-align:center'><span lang=EN-US style='mso-no-proof:
yes'><![if !vml]><img border=0 width=762 height=549
src="../images_file/Cross-season%20Localization.files/image018.jpg" v:shapes="図_x0020_40"><![endif]></span></p>

<p><span lang=EN-US>Fig. 8. Comparison across seasons. Horizontal axis: sorted
query ID.Vertical axis: difference of normalized ranks </span>Δ<span
lang=EN-US>r=rS1?rS2 between different season databases. S1 and S2 are two
different seasons indicated in the key(e.g., </span>“<span lang=EN-US>WI-SP</span>”<span
lang=EN-US> indicates </span>Δ<span lang=EN-US>r = rWI ?rSP).</span></p>

<p><span lang=EN-US><o:p>&nbsp;</o:p></span></p>

<p style='margin-bottom:12.0pt'><span lang=EN-US><o:p>&nbsp;</o:p></span></p>

<p><span lang=EN-US style='mso-no-proof:yes'><![if !vml]><img border=0 width=700 height=416
src="../images_file/Cross-season%20Localization.files/image020.jpg" v:shapes="図_x0020_43"><![endif]></span></p>

<p align=center style='text-align:center'><span lang=EN-US><br>
Fig. 9. Results for NBNN image-to-class distance measure.</span></p>

<p><span lang=EN-US><o:p>&nbsp;</o:p></span></p>

</div>

</body>

</html>
