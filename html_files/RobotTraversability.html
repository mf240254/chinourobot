<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>Robot Traversability Prediction: Towards Third-Person-View Extension of Walk2Map with Photometric and Physical Constraints - IEEE 2024</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      background-color: #ffffff;
      color: #000000;
      line-height: 1.6;
    }
    h1 {
      font-size: 26px;
      font-weight: bold;
    }
    h2 {
      font-size: 22px;
      margin-top: 30px;
      border-bottom: 2px solid #007acc;
      padding-bottom: 5px;
    }
    .authors {
      font-size: 16px;
      margin-bottom: 20px;
    }
    .abstract {
      background-color: #f0f0f0;
      padding: 20px;
      border-left: 5px solid #007acc;
      margin-bottom: 30px;
    }
    .abstract p {
      margin-bottom: 15px;
    }
    .links a {
      display: inline-block;
      margin-right: 15px;
      color: #007acc;
      text-decoration: none;
    }
    .bibtex {
      background-color: #f9f9f9;
      padding: 15px;
      font-family: monospace;
      white-space: pre-wrap;
      border: 1px solid #ccc;
      margin-bottom: 30px;
    }
    .images {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      margin-top: 20px;
    }
    .images img {
      max-width: 100%;
      height: auto;
      border: 1px solid #ccc;
    }
    .keywords {
      background-color: #f9f9f9;
      padding: 10px;
      border: 1px dashed #007acc;
      margin-bottom: 30px;
    }
  </style>
</head>
<body>

  <h1>Robot Traversability Prediction: Towards Third-Person-View Extension of Walk2Map with Photometric and Physical Constraints</h1>

  <div class="authors">
    Jonathan Tay Yu Liang, Kanji Tanaka  </div>

  <h2>Keywords</h2>
  <div class="keywords">

  </div>

  <h2>Abstract</h2>
  <div class="abstract">
    <p>Walk2Map has emerged as a promising data- driven method to generate indoor traversability maps based solely on pedestrian trajectories,
      offering great potential for indoor robot navigation. In this study, we investigate a novel approach called Walk2Map++,
      which involves replacing Walk2Map’s first-person sensor (i.e., IMU) with a human observing third-person view from the robot’s onboard camera.
      However, human observation from a third-person camera is significantly ill-posed due to visual uncertainties resulting from occlusion, nonlinear perspective,
      depth ambiguity, and human- to-human interaction. To regularize the ill-posedness, we pro- pose integrating two types of constraints:
      photometric (i.e., occlusion ordering) and physical (i.e., collision avoidance). We demonstrate that these constraints can be effectively
      inferred from the interaction between past and present observations, human trackers, and object reconstructions.
      We depict the seamless integration of asynchronous map optimization events, like loop closure, into the real-time traversability map,
      facilitat- ing incremental and efficient map refinement. We validate the efficacy of our enhanced methodology through rigorous fusion and comparison with established techniques,
      demonstrating its capability to advance traversability prediction in complex indoor environments. The code and datasets associated with this study are available for further research
      and adoption in the field at https://github.com/jonathantyl97/HO3-SLAM.</p>
  </div>

  <h2>Related documents</h2>
  <div class="links">
    <a href="論文PDFへのリンク">[PDF]</a>
    <a href="補足資料へのリンク">[Supplementary]</a>
    <a href="arXivへのリンク">[arXiv]</a>
    <a href="コードへのリンク">[Code]</a>
  </div>

  <h2>BibTeX</h2>
  <div class="bibtex">
@inproceedings{DBLP:conf/iros/Liang024,
  author       = {Jonathan Tay Yu Liang and
                  Kanji Tanaka},
  title        = {Robot Traversability Prediction: Towards Third-Person-View Extension
                  of Walk2Map with Photometric and Physical Constraints},
  booktitle    = {{IEEE/RSJ} International Conference on Intelligent Robots and Systems,
                  {IROS} 2024, Abu Dhabi, United Arab Emirates, October 14-18, 2024},
  pages        = {11602--11609},
  publisher    = {{IEEE}},
  year         = {2024},
  url          = {https://doi.org/10.1109/IROS58592.2024.10802356},
  doi          = {10.1109/IROS58592.2024.10802356},
  timestamp    = {Fri, 03 Jan 2025 11:28:36 +0100},
  biburl       = {https://dblp.org/rec/conf/iros/Liang024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
  </div>

  <h2>図表・写真</h2>
  <div style="text-align: left;">
      <img src="../images_file/RobotTraversabilityPrediction/image1.png" alt="Fig1" style="max-width: 60%;">
      <p>Fig. 1. Vision-based traversability prediction is an open problem in crowded office environments where occlusions and obstacles are rich.
        In a crowded dynamic scene, it is difficult to obtain a good point cloud map and human detection due to occlusions and obstacles.
        The left and right panels show the DSO point cloud map and Detectron2 human detection mask, respectively.
    </p></div>
  
  <div style="text-align: left;">
      <img src="../images_file/RobotTraversabilityPrediction/image2.png" alt="Fig2" style="max-width: 60%;">
      <p>Fig. 2. Block diagram of framework: All modules are interconnected via ROS (Robot Operating System). The PHYS module comprises DSO (Direct Sparse Odometry) 
        and is responsible for generating point clouds utilized by other modules. Within the PHOT module, a Human-Object Occlusion Ordering Algorithm is employed to 
        extract occlusion ordering information, which is then combined with point cloud coordinates derived from Detectron2 human masks. Additionally, the Walk2Map++ 
        module utilizes human pose estimation to predict human distance from the camera and estimate traversable regions. These traversability maps are visualized using 
        the rviz visualizer. In the traversability map image, the red box represent the estimated human location, hence the traversable region. The grey path indicates the 
        traversable region, which has been walked by the human. The grey path is inprinted by the red boxes.
    </p></div>

  <div style="text-align: left;">
      <img src="../images_file/RobotTraversabilityPrediction/image3.png" alt="Fig3" style="max-width: 60%;">
      <p>Fig. 3. Traversability prediction under severe occlusion. Left: Conven- tional first-person-view setup with IMU. Right: Proposed third-person-view monocular vision setup.
      </p></div>
  
  <div style="text-align: left;">
      <img src="../images_file/RobotTraversabilityPrediction/image4.png" alt="Fig4" style="max-width: 60%;">
      <p>Fig. 4. Top-left: The projection of point clouds to the keyframes visualize; Top-right: Human mask used for occlusion ordering algorithm; Bottom-left: DSO;
        Bottom-right: Traversability map visualized with rviz visualizer.
      </p></div>

  <div style="text-align: left;">
      <img src="../images_file/RobotTraversabilityPrediction/image5.png" alt="Fig5" style="max-width: 60%;">
      <p>Fig. 5. (Left, Middle): Human-centric coordinate system. As shown in the middle figure, keypoint 1 and keypoint 8 is used as a reference point of 
        torso length.(Right): The relationship between occluder’s feature point and occluded human’s region.
      </p></div>

  <div style="text-align: left;">
      <img src="../images_file/RobotTraversabilityPrediction/image6.png" alt="Fig6" style="max-width: 60%;">
      <p>Fig. 6. Left: Occlusion ordering algorithm; Right: Grouped cluster point cloud visualized
        in rviz visualizer. Purple cluster indicate points that are in front of the human, orange cluster indicate points that are behind of the human.
      </p></div>

  <div style="text-align: left;">
      <img src="../images_file/RobotTraversabilityPrediction/image7.png" alt="Fig7" style="max-width: 60%;">
      <p>Fig. 7. (a) Online Traversability Map: In our traversability map, the red box represents the current camera position while the green box
        represents the estimated human location. The gray region indicates the human trail, hence the traversable region. The black lines indicates
        the obstacles or point cloud clusters. (b) Observer Robot set up, with a right-facing monocular camera mounted on the platform of approx. 
        1m height from ground
      </p></div>

  <div style="text-align: left;">
      <img src="../images_file/RobotTraversabilityPrediction/image8.png" alt="Fig8" style="max-width: 60%;">
      <p>Fig. 8. I-Shape path experimental set up which simulates a crowded indoor scene.
      </p></div>

  <div style="text-align: left;">
      <img src="../images_file/RobotTraversabilityPrediction/image9.png" alt="Fig9" style="max-width: 60%;">
      <p>Fig. 9. Bird’s eye view of obstacles setup of all kinds of configurations, namely I-Configuration, L-Configuration, and T-Configuration. 
        The gray rectangle box indicates the point cloud data from DSO, hence the tables set up. The green trail is the frame positional data from DSO,
        and the red triangle is the current camera position, or current frame. Our data collection process is by using a robot equipped with a monocular
        camera and taking a video footage surrounding the set up.
  </p></div>

  <div style="text-align: left;">
      <p><strong>Table1：</strong>Performance results</p>
      <img src="../images_file/RobotTraversabilityPrediction/table1.png" alt="Table1" style="max-width: 45%;">
  
</body>
</html>
