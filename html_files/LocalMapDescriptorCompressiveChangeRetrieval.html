<p><b><span lang=EN-US style='font-size:24.0pt'>Local Map Descriptor for
Compressive Change Retrieval</span></b><span lang=EN-US><br>
<br>
<b>Abstract</b>-Change detection, i.e., anomaly detection from local maps built
by a mobile robot at multiple different times, is a challenging problem to solve
in practice. Most previous work either cannot be applied to scenarios where the
size of the map collection is large, or simply assumed that the robot
self-location is globally known. In this paper, we tackle the problem of
simultaneous self-localization and change detection, by reformulating the
problem as a map retrieval problem, and propose a local map descriptor with a
compressed bag-of-words (BoW) structure as a scalable solution. We make the
following contributions. (1) To enable a direct comparison of the spatial
layout of visual features between different local maps, the origin of the local
map coordinate (termed </span>“<span lang=EN-US>viewpoint</span>”<span
lang=EN-US>) is planned by scene parsing and determined by our </span>“<span
lang=EN-US>viewpoint planner</span>”<span lang=EN-US> <span class=GramE>to</span>
be invariant against small variations in self-location and changes, aiming at
providing similar viewpoints for similar scenes (i.e., the relevant map pair).
(2) We extend the BoW model to enable the use of not only the appearance (e.g.,
polestar) but also the spatial layout (e.g., spatial pyramid) of visual
features with respect to the planned viewpoint. The key observation is that the
planned viewpoint (i.e., the origin of local map coordinate) acts as a pseudo
viewpoint that is usually required by spatial BoW (e.g., SPM) and also by
anomaly detection (e.g., NN-d, LOF). (3) Experimental results on a challenging </span>“<span
lang=EN-US>loop-closing</span>”<span lang=EN-US> scenario show that the
proposed method outperforms previous BoW methods in self-localization, and
furthermore, <span class=GramE>that</span> the use of both appearance and pose
information in change detection produces better results than the use of either
information alone.<br style='mso-special-character:line-break'>
<![if !supportLineBreakNewLine]><br style='mso-special-character:line-break'>
<![endif]></span></p>

<p><b><span lang=EN-US>Members:</span></b>　<span lang=EN-US>Tanaka Kanji, <span
class=SpellE>Murase</span> <span class=SpellE>Tomoya</span><br>
<br>
<b>Relevant Publication:</b><br>
Tanaka Kanji<br>
Robotics and <span class=SpellE>Biomimetics</span> (ROBIO), 2016 IEEE
International Conference on<br>
Local Map Descriptor for Compressive Change Retrieval <br>
<a href="https://dblp.org/rec/bibtex/conf/robio/Tanaka16">Bibtex source</a>, <a href="https://arxiv.org/abs/1603.00980">Document PDF</a><br>
<br>
<br>
<b>Acknowledgements: </b>This work is supported in part by JSPS KAKENHI
Grant-in-Aid for Young Scientists (B) 23700229, and for Scientific Research (C)
26330297.</span></p>

<p><span lang=EN-US><br>
<br>
<span style='mso-no-proof:yes'><img border=0 width=389 height=506
src="../images_file/LocalMapDescriptor2/image002.jpg" v:shapes="図_x0020_13"><![endif]></span><br>
<br>
Fig. 1. The key idea is viewpoint planning in which the origin of the local map
coordinate (termed </span>“<span lang=EN-US>viewpoint</span>”<span lang=EN-US>)
is planned by scene parsing and determined by our </span>“<span lang=EN-US>viewpoint
planner</span>”<span lang=EN-US> to be invariant against small variations in
self-location and changes, which aims at providing similar viewpoints for
similar scenes (i.e., the relevant map pair) and enables a direct comparison of
both the appearance and the pose of visual features between each map pair
(i.e., without requiring pre-alignment of each map pair). (a) A query local map
(left) and a database local map (right) together with the robot</span>’<span
lang=EN-US>s trajectory (red points). (b) Scene parsing results (green line
segments) and planned viewpoint (the big red point). (c) Detected anomaly
(small colored boxes) and anomaly-ness score (color bar).<br>
<br>
<br>
<span style='mso-no-proof:yes'><img border=0 width=1071 height=254
src="../images_file/LocalMapDescriptor2/image004.jpg" v:shapes="図_x0020_16"><![endif]></span><br>
<br>
Fig. 2. The overall pipeline of the algorithm, which involves four main steps:
viewpoint planning (a), local map descriptor (b), global self-localization (c:
database retrieval, d: SPM matching) and change detection (e: anomaly
detection, f: thresholding, g: re-ranking), which are described in sections
III, III-A, III-B, and III-C, respectively. The processes (a)-(g) are indicated
by the arrows </span>“<span lang=EN-US>)</span>”<span lang=EN-US> in the
figure.<br>
<br>
<br>
<span style='mso-no-proof:yes'><img border=0 width=1182 height=314
src="../images_file/LocalMapDescriptor2/image006.jpg" v:shapes="図_x0020_19"><![endif]></span><br>
<br>
Fig. 3. Datasets. (Purple points: point clouds. Green curve: robot</span>’<span
lang=EN-US>s trajectory. Each of the light blue line segments connects a local
map pair that corresponds to each ground-truth loop closing.)<br>
<br>
<br>
<span style='mso-no-proof:yes'><img border=0 width=1115 height=478
src="../images_file/LocalMapDescriptor2/image008.jpg" v:shapes="図_x0020_22"><![endif]></span><br>
<br>
Fig. 4. Samples of scene parsing. 12 (= 3_4) different pairs of scene parsing
are shown for 12 relevant pairings of a query local map (left) and a relevant
database local map (right) with </span>“<span lang=EN-US>query map</span>’<span
lang=EN-US>s ID, database map</span>’<span lang=EN-US>s ID</span>”<span
lang=EN-US>. (Orange points: datapoints from the original local map. Boxes: </span>“<span
lang=EN-US>room</span>”<span lang=EN-US> primitives proposed by the CoR method.
The green big circle: planned viewpoint. Small blue points with lines: the
robot</span>’<span lang=EN-US>s trajectory. Green dots: unoccupied cells.)<br>
<br>
<br>
<br>
<span style='mso-no-proof:yes'><img border=0 width=1055 height=502
src="../images_file/LocalMapDescriptor2/image010.jpg" v:shapes="図_x0020_25"><![endif]></span></span></p>

<p><span lang=EN-US style='mso-no-proof:yes'><img border=0 width=1121 height=470
src="../images_file/LocalMapDescriptor2/image012.jpg" v:shapes="図_x0020_28"><![endif]></span><span
lang=EN-US><br>
<br>
<br>
<br>
<br>
Fig. 5. Change detection. Point clouds of the query and relevant database maps
and ground-truth changes (light blue points) are overlaid using the information
of the planned viewpoint, and are shown with </span>“<span lang=EN-US>query map</span>’<span
lang=EN-US>s ID, database map</span>’<span lang=EN-US>s ID</span>”<span
lang=EN-US>. The meaning of the datapoints, anomalies, and anomaly-ness is the
same as in Fig.1.<br>
<br>
<br>
<span style='mso-no-proof:yes'><img border=0 width=567 height=234
src="../images_file/LocalMapDescriptor2/image014.jpg" v:shapes="図_x0020_31"><img border=0 width=562 height=234
src="../images_file/LocalMapDescriptor2/image016.jpg" v:shapes="図_x0020_34"><![endif]></span><br>
<br>
Fig. 6. Self-localization performance. (Vertical axis: ANR[%]. For each
dataset, from left to right, results for descriptor #1, ..., #8 are shown. )<br>
<br>
<br>
<span style='mso-no-proof:yes'><img border=0 width=874 height=476
src="../images_file/LocalMapDescriptor2/image018.jpg" v:shapes="図_x0020_37"><![endif]></span></span></p>

<p><span lang=EN-US style='mso-no-proof:yes'><img border=0 width=660 height=476
src="../images_file/LocalMapDescriptor2/image020.jpg" v:shapes="図_x0020_40"><![endif]></span><span
lang=EN-US><br>
<br>
Fig. 7. Change detection performance (LMD+CoG, descriptor#1). Horizontal axis:
rank of change mask (log-scale). Vertical axis: recognition rate.<br>
<br>
<br>
TABLE I<br>
TOP-X % RECOGNITION RATE FOR 2,590 RETRIEVAL EXPEWRIMENTS<br>
<span style='mso-no-proof:yes'><img border=0 width=727 height=160
src="../images_file/LocalMapDescriptor2/image022.jpg" v:shapes="図_x0020_43"><![endif]></span><br>
<br>
<br style='mso-special-character:line-break'>
<![if !supportLineBreakNewLine]><br style='mso-special-character:line-break'>
<![endif]></span></p>

</div>

</body>

</html>
