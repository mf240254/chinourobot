<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>Fine-Grained Self-Localization from Coarse Egocentric Topological Maps - VISAPP 2025</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      background-color: #ffffff;
      color: #000000;
      line-height: 1.6;
    }
    h1 {
      font-size: 26px;
      font-weight: bold;
    }
    h2 {
      font-size: 22px;
      margin-top: 30px;
      border-bottom: 2px solid #007acc;
      padding-bottom: 5px;
    }
    .authors {
      font-size: 16px;
      margin-bottom: 20px;
    }
    .abstract {
      background-color: #f0f0f0;
      padding: 20px;
      border-left: 5px solid #007acc;
      margin-bottom: 30px;
    }
    .abstract p {
      margin-bottom: 15px;
    }
    .links a {
      display: inline-block;
      margin-right: 15px;
      color: #007acc;
      text-decoration: none;
    }
    .bibtex {
      background-color: #f9f9f9;
      padding: 15px;
      font-family: monospace;
      white-space: pre-wrap;
      border: 1px solid #ccc;
      margin-bottom: 30px;
    }
    .images {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      margin-top: 20px;
    }
    .images img {
      max-width: 100%;
      height: auto;
      border: 1px solid #ccc;
    }
    .keywords {
      background-color: #f9f9f9;
      padding: 10px;
      border: 1px dashed #007acc;
      margin-bottom: 30px;
    }
  </style>
</head>
<body>

  <h1>Fine-Grained Self-Localization from Coarse Egocentric Topological Maps</h1>

  <div class="authors">
    Daiki Iwata, Kanji Tanaka, Mitsuki Yoshida, Ryogo Yamamoto, Yuudai Morishita and Tomoe Hiroki  </div>

  <h2>Keywords</h2>
  <div class="keywords">
    Active Topological Navigation, Ego-Centric Topological Maps, Incremental Planner Retraining
  </div>

  <h2>Abstract</h2>
  <div class="abstract">
    <p>Topological maps are increasingly favored in robotics for their cognitive relevance, compact storage, and ease of transferability to human users. While these maps provide scalable solutions for navigation and action planning, they present challenges for tasks requiring fine-grained self-localization, such as object goal navigation. This paper investigates the action planning problem of active self-localization from a novel perspective: can an action planner be trained to achieve fine-grained self-localization using coarse topological maps? Our approach acknowledges the inherent limitations of topological maps; overly coarse maps lack essential information for action planning, while excessively high-resolution maps diminish the need for an action planner. To address these challenges, we propose the use of egocentric topological maps to capture fine scene variations. This representation enhances self-localization accuracy by integrating an output probability map as a place-specific score vector into the action planner as a fixed-length state vector. By leveraging sensor data and action feedback, our system optimizes self-localization performance. For the experiments, the de facto standard particle filter-based sequential self-localization framework was slightly modified to enable the transformation of ranking results from a graph convolutional network (GCN)-based topological map classifier into real-valued vector state inputs by utilizing bag-of-place-words and reciprocal rank embeddings. Experimental validation of our method was conducted in the Habitat workspace, demonstrating the potential for effective action planning using coarse maps.</p>
  </div>

  <h2>Related document</h2>
  <div class="links">
    <a href="https://www.scitepress.org/Papers/2025/130980/130980.pdf">[PDF]</a>
    <a href="補足資料へのリンク">[Supplementary]</a>
    <a href="arXivへのリンク">[arXiv]</a>
    <a href="コードへのリンク">[Code]</a>
  </div>

  <h2>BibTeX</h2>
  <div class="bibtex">
@inproceedings{DBLP:conf/visigrapp/Iwata0YYYT25,
  author       = {Daiki Iwata and
                  Kanji Tanaka and
                  Mitsuki Yoshida and
                  Ryogo Yamamoto and
                  Morishita Yuudai and
                  Hiroki Tomoe},
  editor       = {Thomas Bashford{-}Rogers and
                  Daniel Meneveaux and
                  Mehdi Ammi and
                  Mounia Ziat and
                  Stefan J{\"{a}}nicke and
                  Helen C. Purchase and
                  Petia Radeva and
                  Antonino Furnari and
                  Kadi Bouatouch and
                  A. Augusto de Sousa},
  title        = {Fine-Grained Self-Localization from Coarse Egocentric Topological
                  Maps},
  booktitle    = {Proceedings of the 20th International Joint Conference on Computer
                  Vision, Imaging and Computer Graphics Theory and Applications, {VISIGRAPP}
                  2025 - Volume 2: VISAPP, Porto, Portugal, February 26-28, 2025},
  pages        = {810--819},
  publisher    = {{SCITEPRESS}},
  year         = {2025},
  url          = {https://doi.org/10.5220/0013098000003912},
  doi          = {10.5220/0013098000003912},
  timestamp    = {Tue, 15 Apr 2025 11:34:20 +0200},
  biburl       = {https://dblp.org/rec/conf/visigrapp/Iwata0YYYT25.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
  </div>

  <h2>図表・写真</h2>
  <div style="text-align: left;">
    <img src="../images_file/Fine-GrainedSelf-Localization/image1.png" alt="Fig1" style="max-width: 60%;">
    <p>
    Figure 1: Topological navigation using ego-centric topological maps. 
    Left: Conventional world-centric map. Right:The proposed ego-centric map.
    </p></div>

  <div style="text-align: left;">
        <img src="../images_file/Fine-GrainedSelf-Localization/image2.png" alt="Fig2" style="max-width: 60%;">
        <p>Figure 2: Scene parsing. Left: SGB (Tang et al., 2020). Right: Ours.</p>
    </div>
    
  <div style="text-align: left;">
        <img src="../images_file/Fine-GrainedSelf-Localization/image3.png" alt="Fig3" style="max-width: 60%;">
        <p>Figure 3: Seven spatially adjacent input images along the robot trajectory and their bag-of-words representation: The graph shows the time difference of bag-of-words histogram h[t](t = 1,2,3,4,5,6,7) of viewpoint sequence of length 7. ∆h[t]=h[t +1]−h[t](t=1,2,3,4,5,6). Among the elements of ∆h[t], three visual words with small absolute values of time difference are chosen, and their prototype ego-centric topological maps are shown in the figure.
    </p></div>
  
  <div style="text-align: left;">
        <img src="../images_file/Fine-GrainedSelf-Localization/image4.png" alt="Fig4" style="max-width: 60%;">
        <p>Figure 4: Experimental environments.
    </p></div>
  
  <div style="text-align: left;">
    <p><strong>Table1：</strong>Performance results.</p>
    <img src="../images_file/Fine-GrainedSelf-Localization/table1.png" alt="Table1" style="max-width: 45%;">
  </div>

  <div style="text-align: left;">
        <img src="../images_file/Fine-GrainedSelf-Localization/image5.png" alt="Fig5" style="max-width: 60%;">
        <p>Figure 5: Time cost per sense-plan-acition cycle. A: Preprocessing. B: Particle filter.
         C: Action planning. D: Planner retraining.
    </p></div>

  <div style="text-align: left;">
        <img src="../images_file/Fine-GrainedSelf-Localization/image6.png" alt="Fig6" style="max-width: 60%;">
        <p>Figure 6: Examples of L repetitions of sense-plan-action cycles.
    </p></div>

</body>
</html>
