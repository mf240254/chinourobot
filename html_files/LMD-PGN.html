<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>LMD-PGN:Cross-Modal Knowledge Distillation from First-Person-View Images to Third-Person-View BEV Maps for Universal Point Goal Navigation - CoRR</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      background-color: #ffffff;
      color: #000000;
      line-height: 1.6;
    }
    h1 {
      font-size: 26px;
      font-weight: bold;
    }
    h2 {
      font-size: 22px;
      margin-top: 30px;
      border-bottom: 2px solid #007acc;
      padding-bottom: 5px;
    }
    .authors {
      font-size: 16px;
      margin-bottom: 20px;
    }
    .abstract {
      background-color: #f0f0f0;
      padding: 20px;
      border-left: 5px solid #007acc;
      margin-bottom: 30px;
    }
    .abstract p {
      margin-bottom: 15px;
    }
    .links a {
      display: inline-block;
      margin-right: 15px;
      color: #007acc;
      text-decoration: none;
    }
    .bibtex {
      background-color: #f9f9f9;
      padding: 15px;
      font-family: monospace;
      white-space: pre-wrap;
      border: 1px solid #ccc;
      margin-bottom: 30px;
    }
    .images {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      margin-top: 20px;
    }
    .images img {
      max-width: 100%;
      height: auto;
      border: 1px solid #ccc;
    }
    .keywords {
      background-color: #f9f9f9;
      padding: 10px;
      border: 1px dashed #007acc;
      margin-bottom: 30px;
    }
  </style>
</head>
<body>

  <h1>LMD-PGN:Cross-Modal Knowledge Distillation from First-Person-View Images to Third-Person-View BEV Maps for Universal Point Goal Navigation</h1>

  <div class="authors">
    Riku Uemura, Kanji Tanaka, Kenta Tsukahara, Daiki Iwata  </div>

  <h2>Keywords</h2>
  <div class="keywords">

  </div>

  <h2>Abstract</h2>
  <div class="abstract">
    <p>Point goal navigation (PGN) is a mapless naviga- tion approach that trains robots to visually navigate to goal points without relying on pre-built maps. Despite significant progress in handling complex environments using deep rein- forcement learning, current PGN methods are designed for single-robot systems, limiting their generalizability to multi- robot scenarios with diverse platforms. This paper addresses this limitation by proposing a knowledge transfer framework for PGN, allowing a teacher robot to transfer its learned navigation model to student robots, including those with un- known or black-box platforms. We introduce a novel knowledge distillation (KD) framework that transfers first-person-view (FPV) representations (view images, turning/forward actions) to universally applicable third-person-view (TPV) representations (local maps, subgoals). The state is redefined as reconstructed local maps using SLAM, while actions are mapped to subgoals on a predefined grid. To enhance training efficiency, we propose a sampling-efficient KD approach that aligns training episodes via a noise-robust local map descriptor (LMD). Although validated on 2D wheeled robots, this method can be extended to 3D action spaces, such as drones. Experiments conducted in Habitat-Sim demonstrate the feasibility of the proposed framework, requiring minimal implementation effort. This study highlights the potential for scalable and cross-platform PGN solutions, expanding the applicability of embodied AI systems in multi-robot scenarios.</p>
  </div>

  <h2>Related documents</h2>
  <div class="links">
    <a href="論文PDFへのリンク">[PDF]</a>
    <a href="補足資料へのリンク">[Supplementary]</a>
    <a href=https://arxiv.org/abs/2412.17282>[arXiv]</a>
    <a href="コードへのリンク">[Code]</a>
  </div>

  <h2>BibTeX</h2>
  <div class="bibtex">
@article{DBLP:journals/corr/abs-2412-17282,
  author       = {Riku Uemura and
                  Kanji Tanaka and
                  Kenta Tsukahara and
                  Daiki Iwata},
  title        = {{LMD-PGN:} Cross-Modal Knowledge Distillation from First-Person-View
                  Images to Third-Person-View {BEV} Maps for Universal Point Goal Navigation},
  journal      = {CoRR},
  volume       = {abs/2412.17282},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2412.17282},
  doi          = {10.48550/ARXIV.2412.17282},
  eprinttype    = {arXiv},
  eprint       = {2412.17282},
  timestamp    = {Fri, 24 Jan 2025 21:54:26 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2412-17282.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
  </div>

  <h2>図表・写真</h2>
  <div class="images">
    <img src="../images_file/LMD-PGN/image1.png" width="500" alt="Fig1">
    <figcaption><p>
    Fig.1. Example of a robot’s movement trajectory. 
    The red dot represents the starting point, the blue curve represents the movement path, 
    and the green circle represents the goal area.</p></figcaption></div>
  
  <div class="images">
    <img src="../images_file/LMD-PGN/image2.png" width="500" alt="Fig2">
    <figcaption><p>
    Fig. 2. We adopted a BEV (Bird’s Eye View) omnidirectional local map,
    as shown in the figure, with the priority of being independent of specific platforms.
    </p></figcaption></div>

　<div style="text-align: left;">
        <img src="../images_file/LMD-PGN/image2.png" alt="Fig2" style="max-width: 80%;">
        <p>Fig. 2. We adopted a BEV (Bird’s Eye View) omnidirectional local map,
    as shown in the figure, with the priority of being independent of specific platforms.</p>
    </div>
  
  <div style="text-align: left;">
    <p><strong>Table1：</strong>Table.1 PERFORMANCE RESULTS (ACHIEVEMENT RATE [%])</p>
    <img src="../images_file/LMD-PGN/table1.png" alt="Table1" style="max-width: 100%;">
  </div>

</body>
</html>
