<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>Mining Visual Phrases for Long-Term Visual SLAM - IROS 2014 </title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      background-color: #ffffff;
      color: #000000;
      line-height: 1.6;
    }
    h1 {
      font-size: 26px;
      font-weight: bold;
    }
    h2 {
      font-size: 22px;
      margin-top: 30px;
      border-bottom: 2px solid #007acc;
      padding-bottom: 5px;
    }
    .authors {
      font-size: 16px;
      margin-bottom: 20px;
    }
    .abstract {
      background-color: #f0f0f0;
      padding: 20px;
      border-left: 5px solid #007acc;
      margin-bottom: 30px;
    }
    .abstract p {
      margin-bottom: 15px;
    }
    .links a {
      display: inline-block;
      margin-right: 15px;
      color: #007acc;
      text-decoration: none;
    }
    .bibtex {
      background-color: #f9f9f9;
      padding: 15px;
      font-family: monospace;
      white-space: pre-wrap;
      border: 1px solid #ccc;
      margin-bottom: 30px;
    }
    .images {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      margin-top: 20px;
    }
    .images img {
      max-width: 50%;
      height: auto;
      border: 1px solid #ccc;
    }
    .keywords {
      background-color: #f9f9f9;
      padding: 10px;
      border: 1px dashed #007acc;
      margin-bottom: 30px;
    }
  </style>
</head>
<body>

  <h1>Mining Visual Phrases for Long-Term Visual SLAM</h1>

  <div class="authors">
    Tanaka Kanji, Chokushi Yuuto, Ando Masatoshi  </div>

  <h2>Keywords</h2>
  <div class="keywords">
    
  </div>

  <h2>Abstract</h2>
  <div class="abstract">
    <p>We propose a discriminative and compact scene descriptor for single-view place recognition that facilitates long-term visual SLAM in familiar, semi-dynamic and partially changing environments. In contrast to popular bag-ofwords scene descriptors, which rely on a library of vector quantized visual features, our proposed scene descriptor is based on a library of raw image data (such as an available visual experience, images shared by other colleague robots, and publicly available image data on the web) and directly mine it to find visual phrases (VPs) that discriminatively and compactly explain an input query / database image. Our mining approach is motivated by recent success in the field of common pattern discovery—specifically mining of common visual patterns among scenes—and requires only a single library of raw images that can be acquired at different time or day. Experimental results show that even though our scene descriptor is significantly more compact than conventional descriptors it has a relatively higher recognition performance.</p>
  </div>

  <h2>Related document</h2>
  <div class="links">
    <a href="">[PDF]</a>
    <a href="補足資料へのリンク">[Supplementary]</a>
    <a href="">[arXiv]</a>
    <a href="コードへのリンク">[Code]</a>
  </div>

  <h2>BibTeX</h2>
  <div class="bibtex">
@inproceedings{DBLP:conf/iros/TanakaCA14,
  author       = {Kanji Tanaka and
                  Yuuto Chokushi and
                  Masatoshi Ando},
  title        = {Mining visual phrases for long-term visual {SLAM}},
  booktitle    = {2014 {IEEE/RSJ} International Conference on Intelligent Robots and
                  Systems, {IROS} 2014, Chicago, IL, USA, September 14-18, 2014},
  pages        = {136--142},
  publisher    = {{IEEE}},
  year         = {2014},
  url          = {https://doi.org/10.1109/IROS.2014.6942552},
  doi          = {10.1109/IROS.2014.6942552},
  timestamp    = {Mon, 25 Sep 2023 20:37:18 +0200},
  biburl       = {https://dblp.org/rec/conf/iros/TanakaCA14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
  </div>

  <h2>図表・写真</h2>
  <div class="images">
    <img src="../images_file//image1.png" alt="Fig. 1">
      <figcaption>.</figcaption>
    <img src="../images_file//image2.png" alt="図2 説明">
<figcaption>a</figcaption>
  </div>

</body>
</html>
