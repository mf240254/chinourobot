<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>Active Domain-Invariant Self-Localization Using Ego-Centric and World-Centric Maps - CVMI 2022 </title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      background-color: #ffffff;
      color: #000000;
      line-height: 1.6;
    }
    h1 {
      font-size: 26px;
      font-weight: bold;
    }
    h2 {
      font-size: 22px;
      margin-top: 30px;
      border-bottom: 2px solid #007acc;
      padding-bottom: 5px;
    }
    .authors {
      font-size: 16px;
      margin-bottom: 20px;
    }
    .abstract {
      background-color: #f0f0f0;
      padding: 20px;
      border-left: 5px solid #007acc;
      margin-bottom: 30px;
    }
    .abstract p {
      margin-bottom: 15px;
    }
    .links a {
      display: inline-block;
      margin-right: 15px;
      color: #007acc;
      text-decoration: none;
    }
    .bibtex {
      background-color: #f9f9f9;
      padding: 15px;
      font-family: monospace;
      white-space: pre-wrap;
      border: 1px solid #ccc;
      margin-bottom: 30px;
    }
    .images {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      margin-top: 20px;
    }
    .images img {
      max-width: 50%;
      height: auto;
      border: 1px solid #ccc;
    }
    .keywords {
      background-color: #f9f9f9;
      padding: 10px;
      border: 1px dashed #007acc;
      margin-bottom: 30px;
    }
  </style>
</head>
<body>

  <h1>Active Domain-Invariant Self-Localization Using Ego-Centric and World-Centric Maps ⋆</h1>

  <div class="authors">
    Kanya Kurauchi, Kanji Tanaka, Ryogo Yamamoto, and Mitsuki Yoshida  </div>

  <h2>Keywords</h2>
  <div class="keywords">
    Visual robot place recognition, Domain-invariant next-best-view planner, Transferring convnet features
  </div>

  <h2>Abstract</h2>
  <div class="abstract">
    <p>The training of a next-best-view (NBV) planner for visual place recognition (VPR) is a fundamentally important task 
      in autonomous robot navigation, for which a typical approach is the use of visual experi- ences that are collected in 
      the target domain as training data. However, the collection of a wide variety of visual experiences in everyday nav- igation is 
      costly and prohibitive for real-time robotic applications. We address this issue by employing a novel domain-invariant NBV planner. 
      A standard VPR subsystem based on a convolutional neural network (CNN) is assumed to be available, and its domain-invariant state recognition
      ability is proposed to be transferred to train the domain-invariant NBV planner. Specifically, we divide the visual cues that are available
      from the CNN model into two types: the output layer cue (OLC) and intermediate layer cue (ILC). The OLC is available at the output layer of the 
      CNN model and aims to estimate the state of the robot (e.g., the robot viewpoint) with respect to the world-centric view coordinate sys- tem. The ILC 
      is available within the middle layers of the CNN model as a high-level description of the visual content (e.g., a saliency image) with respect to the 
      ego-centric view. In our framework, the ILC and OLC are mapped to a state vector and subsequently used to train a multiview NBV planner via deep reinforcement learning.
      Experiments using the public NCLT dataset validate the effectiveness of the proposed method.</p>
  </div>

  <h2>Related document</h2>
  <div class="links">
    <a href="論文PDFへのリンク">[PDF]</a>
    <a href="補足資料へのリンク">[Supplementary]</a>
    <a href="https://arxiv.org/abs/2204.10497">[arXiv]</a>
    <a href="コードへのリンク">[Code]</a>
  </div>

  <h2>BibTeX</h2>
  <div class="bibtex">
@inproceedings{DBLP:conf/cvmi/Kurauchi0YY22,
  author       = {Kanya Kurauchi and
                  Kanji Tanaka and
                  Ryogo Yamamoto and
                  Mitsuki Yoshida},
  editor       = {Massimo Tistarelli and
                  Shiv Ram Dubey and
                  Satish Kumar Singh and
                  Xiaoyi Jiang},
  title        = {Active Domain-Invariant Self-localization Using Ego-Centric and World-Centric
                  Maps},
  booktitle    = {Computer Vision and Machine Intelligence - Proceedings of {CVMI} 2022,
                  {IIIT} Allahabad, India, August 2022},
  series       = {Lecture Notes in Networks and Systems},
  volume       = {586},
  pages        = {475--487},
  publisher    = {Springer},
  year         = {2022},
  url          = {https://doi.org/10.1007/978-981-19-7867-8\_38},
  doi          = {10.1007/978-981-19-7867-8\_38},
  timestamp    = {Tue, 05 Dec 2023 17:22:55 +0100},
  biburl       = {https://dblp.org/rec/conf/cvmi/Kurauchi0YY22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
  </div>

  <h2>図表・写真</h2>
  <div class="images">
    <img src="../images_file/ActiveDomainInvariant/image1.png" alt="Fig. 1">
      <figcaption>Fig. 1. The training of a next-best-view (NBV) planner for visual robot place recogni-
tion (VPR) is fundamentally important for autonomous robot navigation. In this study,
instead of the common path of training from visual experiences, we exploit a standard
single-view VPR model using a deep CNN as the source of training data. Specifically,
we divide the visual cues that are available from the CNN model into two types: the
OLC and ILC, and fuse the OLC and ILC into a new state vector to reformulate the
NBV planning as a domain-invariant task. Heat maps are overlaid on the images to
visualize the OLC/ILC values.</figcaption>
    <img src="../images_file/ActiveDomainInvariant/image2.png" alt="図2 説明">
<figcaption>a</figcaption>
  </div>

</body>
</html>
